{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"movenet_lightning\"\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image.\n",
    "image_path = r'D:\\Pemrograman\\Python\\Project\\Hidup-Sehat-Machine-Learning\\PoseEstimation\\images\\stress_relief\\Easy-Pose_Andrew-Clark_1.jpg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.image.decode_jpeg(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_on_image(\n",
    "    image, person, crop_region=None, close_figure=True,\n",
    "    keep_input_size=False):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    " \n",
    "  Args:\n",
    "    image: An numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    person: A person entity returned from the MoveNet.SinglePose model.\n",
    "    close_figure: Whether to close the plt figure after the function returns.\n",
    "    keep_input_size: Whether to keep the size of the input image.\n",
    " \n",
    "  Returns:\n",
    "    An numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  # Draw the detection result on top of the image.\n",
    "  image_np = utils.visualize(image, [person])\n",
    "  \n",
    "  # Plot the image with detection results.\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  im = ax.imshow(image_np)\n",
    " \n",
    "  if close_figure:\n",
    "    plt.close(fig)\n",
    " \n",
    "  if not keep_input_size:\n",
    "    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
    "\n",
    "  return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.utils' has no attribute 'visualize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m display_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m display_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize_with_pad(\n\u001b[0;32m     11\u001b[0m     display_image, \u001b[39m1280\u001b[39m, \u001b[39m1280\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[1;32m---> 12\u001b[0m output_overlay \u001b[39m=\u001b[39m draw_prediction_on_image(\n\u001b[0;32m     13\u001b[0m     np\u001b[39m.\u001b[39;49msqueeze(display_image\u001b[39m.\u001b[39;49mnumpy(), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), keypoints_with_scores)\n\u001b[0;32m     15\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m     16\u001b[0m plt\u001b[39m.\u001b[39mimshow(output_overlay)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mdraw_prediction_on_image\u001b[1;34m(image, person, crop_region, close_figure, keep_input_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"Draws the keypoint predictions on image.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m  image overlaid with keypoint predictions.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Draw the detection result on top of the image.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m image_np \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mvisualize(image, [person])\n\u001b[0;32m     20\u001b[0m \u001b[39m# Plot the image with detection results.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m height, width, channel \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.utils' has no attribute 'visualize'"
     ]
    }
   ],
   "source": [
    "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "input_image = tf.expand_dims(image, axis=0)\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "# Run model inference.\n",
    "keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "# Visualize the predictions with image.\n",
    "display_image = tf.expand_dims(image, axis=0)\n",
    "display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    display_image, 1280, 1280), dtype=tf.int32)\n",
    "output_overlay = draw_prediction_on_image(\n",
    "    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(output_overlay)\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import TF and TF Hub libraries.\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# # Load the input image.\n",
    "# image_path = r'D:\\Pemrograman\\Python\\Project\\Pose Estimation\\poses_images_out_train\\adho mukha svanasana\\1. 1.png'\n",
    "# image = tf.io.read_file(image_path)\n",
    "# image = tf.compat.v1.image.decode_jpeg(image)[:,:,:3]\n",
    "# image = tf.expand_dims(image, axis=0)\n",
    "# # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "# image = tf.cast(tf.image.resize_with_pad(image, 192, 192), dtype=tf.int32)\n",
    "\n",
    "# # Download the model from TF Hub.\n",
    "# model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "# movenet = model.signatures['serving_default']\n",
    "\n",
    "# # Run model inference.\n",
    "# outputs = movenet(image)\n",
    "# # Output is a [1, 1, 17, 3] tensor.\n",
    "# keypoints = outputs['output_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 17 keypoints in the model output\n",
    "# kp_descriptions = [\n",
    "#     'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "#     'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "#     'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "#     'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "# ]\n",
    "# # Rename columns in the DataFrames according to the values\n",
    "# columns = []\n",
    "# for point in kp_descriptions:\n",
    "#     for value in ('y', 'x', 'score'):\n",
    "#         columns.append(f'{point}_{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.array(keypoints[0][0]).flatten().reshape(1,-1),columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [0.58076525 0.42730328 0.67503864 0.5792289  0.41207743 0.47333997\n",
      " 0.58185965 0.41216052 0.52444524 0.5445528  0.39484406 0.45617175\n",
      " 0.5446289  0.3936121  0.46745512 0.5016635  0.4046818  0.7063899\n",
      " 0.49563456 0.40258092 0.54718405 0.60904396 0.34574008 0.7440503\n",
      " 0.5912181  0.34356624 0.6541515  0.70214546 0.26372153 0.9110583\n",
      " 0.6600009  0.27307063 0.38330933 0.32035175 0.6203919  0.61942005\n",
      " 0.32266206 0.60961163 0.76251876 0.4952395  0.7256286  0.6774133\n",
      " 0.4977281  0.70222014 0.80471295 0.6468238  0.8216442  0.75596136\n",
      " 0.62307787 0.78378725 0.60452706]\n",
      "B: [0.5749743  0.42486113 0.54318196 0.57792485 0.40930325 0.5496756\n",
      " 0.5782155  0.40489376 0.57750416 0.5340253  0.38575894 0.5966261\n",
      " 0.5361736  0.37668896 0.54088706 0.50579745 0.43160397 0.6138604\n",
      " 0.48600656 0.37216204 0.6575917  0.63704824 0.33847758 0.64200926\n",
      " 0.587188   0.31143132 0.7014244  0.72440684 0.2353007  0.6007997\n",
      " 0.6784536  0.23021728 0.51602674 0.30333444 0.6349661  0.74985135\n",
      " 0.31460917 0.60921407 0.68592465 0.47325823 0.77171266 0.6738131\n",
      " 0.48240095 0.73322797 0.8058868  0.6358363  0.84106433 0.6906332\n",
      " 0.6143594  0.7967786  0.7987496 ]\n",
      "Cosine Similarity: 0.9919049110457753\n"
     ]
    }
   ],
   "source": [
    "# # import required libraries\n",
    "# import numpy as np\n",
    "# from numpy.linalg import norm\n",
    " \n",
    "# # define two lists or array\n",
    "# A = np.array([0.58076525, 0.42730328, 0.67503864, 0.5792289 , 0.41207743,\n",
    "#        0.47333997, 0.58185965, 0.41216052, 0.52444524, 0.5445528 ,\n",
    "#        0.39484406, 0.45617175, 0.5446289 , 0.3936121 , 0.46745512,\n",
    "#        0.5016635 , 0.4046818 , 0.7063899 , 0.49563456, 0.40258092,\n",
    "#        0.54718405, 0.60904396, 0.34574008, 0.7440503 , 0.5912181 ,\n",
    "#        0.34356624, 0.6541515 , 0.70214546, 0.26372153, 0.9110583 ,\n",
    "#        0.6600009 , 0.27307063, 0.38330933, 0.32035175, 0.6203919 ,\n",
    "#        0.61942005, 0.32266206, 0.60961163, 0.76251876, 0.4952395 ,\n",
    "#        0.7256286 , 0.6774133 , 0.4977281 , 0.70222014, 0.80471295,\n",
    "#        0.6468238 , 0.8216442 , 0.75596136, 0.62307787, 0.78378725,\n",
    "#        0.60452706])\n",
    "\n",
    "# B = np.array([0.5749743 , 0.42486113, 0.54318196, 0.57792485, 0.40930325,\n",
    "#        0.5496756 , 0.5782155 , 0.40489376, 0.57750416, 0.5340253 ,\n",
    "#        0.38575894, 0.5966261 , 0.5361736 , 0.37668896, 0.54088706,\n",
    "#        0.50579745, 0.43160397, 0.6138604 , 0.48600656, 0.37216204,\n",
    "#        0.6575917 , 0.63704824, 0.33847758, 0.64200926, 0.587188  ,\n",
    "#        0.31143132, 0.7014244 , 0.72440684, 0.2353007 , 0.6007997 ,\n",
    "#        0.6784536 , 0.23021728, 0.51602674, 0.30333444, 0.6349661 ,\n",
    "#        0.74985135, 0.31460917, 0.60921407, 0.68592465, 0.47325823,\n",
    "#        0.77171266, 0.6738131 , 0.48240095, 0.73322797, 0.8058868 ,\n",
    "#        0.6358363 , 0.84106433, 0.6906332 , 0.6143594 , 0.7967786 ,\n",
    "#        0.7987496 ])\n",
    " \n",
    "# print(\"A:\", A)\n",
    "# print(\"B:\", B)\n",
    " \n",
    "# # compute cosine similarity\n",
    "# cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "# print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58076525, 0.42730328, 0.67503864, 0.5792289 , 0.41207743,\n",
       "        0.47333997, 0.58185965, 0.41216052, 0.52444524, 0.5445528 ,\n",
       "        0.39484406, 0.45617175, 0.5446289 , 0.3936121 , 0.46745512,\n",
       "        0.5016635 , 0.4046818 , 0.7063899 , 0.49563456, 0.40258092,\n",
       "        0.54718405, 0.60904396, 0.34574008, 0.7440503 , 0.5912181 ,\n",
       "        0.34356624, 0.6541515 , 0.70214546, 0.26372153, 0.9110583 ,\n",
       "        0.6600009 , 0.27307063, 0.38330933, 0.32035175, 0.6203919 ,\n",
       "        0.61942005, 0.32266206, 0.60961163, 0.76251876, 0.4952395 ,\n",
       "        0.7256286 , 0.6774133 , 0.4977281 , 0.70222014, 0.80471295,\n",
       "        0.6468238 , 0.8216442 , 0.75596136, 0.62307787, 0.78378725,\n",
       "        0.60452706]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(keypoints[0][0]).flatten().reshape(1,-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
